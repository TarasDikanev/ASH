{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Tokenize, lemmatize. Direct and inverted indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')\n",
    "from src.utils import LoadDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "an_jokes = json.loads(open('..\\\\data\\\\processed\\\\an_jokes_eq.json', 'r', encoding = 'utf-8').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_list = ['когда-то', 'тогда-то', 'куда-то', 'откуда-то', 'туда-то', 'где-то', 'чей-то', 'какой-то', 'такой-то', 'который-то', \n",
    "           'как-то', 'так-то', 'зачем-то', 'всего-то', 'чего-то', 'отчего-то', 'сколько-то', 'то-то', 'кто-то', 'что-то', 'тот-то', \n",
    "           'тут-то', 'почему-то']\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(text.lower(), language = 'russian')\n",
    "    tokens = [t for t in tokens if t not in (string.punctuation + \"''``«»\")]\n",
    "    tokens2 = []\n",
    "    for t in tokens:\n",
    "        if t[-3:] == '-то' and t not in to_list:\n",
    "            tokens2 += [t[:-3], '-то']\n",
    "        else:\n",
    "            tokens2 += [t]\n",
    "    return tokens2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['жаль',\n",
       " 'что',\n",
       " 'я',\n",
       " 'плохо',\n",
       " 'рисую',\n",
       " '...',\n",
       " 'но',\n",
       " 'представьте',\n",
       " 'себе',\n",
       " 'картину',\n",
       " 'ламборджини',\n",
       " 'дьябло',\n",
       " 'у',\n",
       " 'открытой',\n",
       " 'двери',\n",
       " 'водителя',\n",
       " 'эффектная',\n",
       " 'девушка',\n",
       " '...',\n",
       " 'рядом',\n",
       " 'валяется',\n",
       " 'мужичок',\n",
       " 'весь',\n",
       " 'оборванный',\n",
       " 'и',\n",
       " 'с',\n",
       " 'фингалами',\n",
       " '...',\n",
       " 'и',\n",
       " 'название',\n",
       " 'надавала',\n",
       " '...']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.random.randint(0, len(an_jokes))\n",
    "joke = an_jokes[idx]\n",
    "\n",
    "tokens = tokenize(joke[0])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatize with pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptions = {\n",
    "    'лол': 'лол', 'белой': 'белый', 'хуясе': 'хуясе', 'джинса': 'джинса', 'гент': 'гент', 'ал': 'ал', \n",
    "    'але': 'але', 'алах': 'аллах', 'усы': 'усы', 'усе': 'усе', 'штоле': 'штоле', 'чето': 'чето', 'че': 'че', \n",
    "    'неее': 'неее', 'чтоле': 'чтоле', 'боков': 'бок', 'кароч': 'кароч', 'нете': 'нет_(сеть)', 'парней': 'парень',\n",
    "    'охуевшая': 'охуевший', 'охуевшей': 'охуевший', 'охуевшие': 'охуевший', 'охуевший': 'охуевший', \n",
    "    'охуевшим': 'охуевший', 'охуевшими': 'охуевший', 'охуевших': 'охуевший', 'ахуевшими': 'ахуевший', \n",
    "    'охуел': 'охуеть', 'охуела': 'охуеть', 'охуели': 'охуеть', 'спиздили': 'спиздить', 'долбоеб': 'долбоеб', \n",
    "    'долбоёб': 'долбоеб', 'хуею': 'хуеть', 'ебло': 'ебло', 'ебли': 'ебать', 'пpизывник': 'призывник'\n",
    "}\n",
    "\n",
    "postprocessing = {\n",
    "    'убунт': 'убунту', 'гент': 'генту', 'дебиана': 'дебиан', 'ктулха': 'ктулху', 'вист': 'виста', \n",
    "    'винд': 'винда', 'ютуба': 'ютуб', 'фич': 'фича', 'смайла': 'смайл', 'свича': 'свич', 'хабра': 'хабр', \n",
    "    'хабрахабра': 'хабрахабр', 'одмина': 'одмин', 'линуха': 'линух', 'хаба': 'хаб', 'пинга': 'пинг', \n",
    "    'конфига': 'конфиг', 'козлы': 'козёл', 'нихуй': 'нихуя', 'хуйн': 'хуйня', 'пидора': 'пидор', \n",
    "    'пидоров': 'пидор', 'пиздюля' : 'пиздюли', 'пидором': 'пидор', 'мудаки': 'мудак', 'мудаками': 'мудак', \n",
    "    'мудаком': 'мудак', 'мудака': 'мудак', 'заебал': 'заебать', 'съебал': 'съебать', 'доебал': 'доебать',\n",
    "    'проебали': 'проебать', 'проебал': 'проебать', 'разъебал': 'разъебать', 'заебали': 'заебать', \n",
    "    'ебали': 'ебать', 'уебали': 'уебать', 'ебал': 'ебать', 'выебал': 'выебать', 'наебал': 'наебать', \n",
    "    'сук': 'сука', 'херовин': 'херовина', 'херачил': 'херачить', 'херачили': 'херачить', 'захерачил': 'захерачить', \n",
    "    'отхерачил': 'отхерачить', 'херней': 'херня', 'дохер': 'дохера', 'отпиздим': 'отпиздить', \n",
    "    'спиздил': 'спиздить', 'насрало': 'насрать', 'дибить': 'дибил', 'ахуесть': 'ахуеть', 'пиздюля': 'пиздюли',\n",
    "    'ебета': 'ебать', 'ебут': 'ебать', 'серёг': 'серёга', 'матана': 'матан', 'игнора': 'игнор', 'ям': 'яма', \n",
    "    'джинса': 'джинсы', 'коты': 'кот', 'вейдёр': 'вейдер', 'побежалый': 'побежать', 'похуделый': 'похудеть', \n",
    "    'комаров': 'комар', 'маркета': 'маркет', 'трусик': 'трусики', 'медведа': 'медвед', 'охренель': 'охренеть',\n",
    "    'дот': 'дота', 'капёс': 'капс', 'винампа': 'винамп', 'ир': 'ира', 'анастасий': 'анастасия', 'марин': 'марина', \n",
    "    'дим': 'дима', 'димона': 'димон', 'красных': 'красный', 'поверь': 'поверить', 'живить': 'жить',\n",
    "    'печенек': 'печенька', 'тапка': 'тапок', 'игнора': 'игнор', 'донцов': 'донцова', 'скинхэда': 'скинхэд', \n",
    "    'скарлетта': 'скарлетт', 'боков': 'боковой', 'друган': 'друган', 'друганом': 'друган', 'другана': 'друган', \n",
    "    'мазд': 'мазда', 'каптать': 'капча', 'лежалый': 'лежать', 'оливья': 'оливье', 'парка': 'парк',\n",
    "    'орало': 'орать', 'охренела': 'охренеть', 'основный': 'основной', 'боев': 'боевой', 'тимлида': 'тимлид', \n",
    "    'млина': 'млин', 'вырасти': 'вырастать', 'фот': 'фота', 'другать': 'друган', 'фейспалма': 'фейспалм', \n",
    "    'погуглила': 'погуглить', 'погуголь': 'погугли', 'семейник': 'семейники', 'донестись': 'доноситься', \n",
    "    'тибить': 'тибя', 'уползти': 'уползать', 'смутиться': 'смущаться'\n",
    "}\n",
    "\n",
    "def lemmatize(token):\n",
    "    if token in exceptions:\n",
    "        lemma = exceptions[token]\n",
    "    else:\n",
    "        lemma = morph.parse(token)[0].normal_form\n",
    "        if lemma in postprocessing:\n",
    "            lemma = postprocessing[lemma]\n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['жаль',\n",
       " 'что',\n",
       " 'я',\n",
       " 'плохо',\n",
       " 'рисовать',\n",
       " '...',\n",
       " 'но',\n",
       " 'представить',\n",
       " 'себя',\n",
       " 'картина',\n",
       " 'ламборджинить',\n",
       " 'дьябнуть',\n",
       " 'у',\n",
       " 'открытый',\n",
       " 'дверь',\n",
       " 'водитель',\n",
       " 'эффектный',\n",
       " 'девушка',\n",
       " '...',\n",
       " 'ряд',\n",
       " 'валяться',\n",
       " 'мужичок',\n",
       " 'весь',\n",
       " 'оборванный',\n",
       " 'и',\n",
       " 'с',\n",
       " 'фингал',\n",
       " '...',\n",
       " 'и',\n",
       " 'название',\n",
       " 'надавать',\n",
       " '...']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joke = an_jokes[idx]\n",
    "\n",
    "tokens = tokenize(joke[0])\n",
    "lemmas = [lemmatize(t) for t in tokens]\n",
    "lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatize with pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['жаль',\n",
       " 'что',\n",
       " 'я',\n",
       " 'плохо',\n",
       " 'рисовать',\n",
       " '...',\n",
       " 'но',\n",
       " 'представлять',\n",
       " 'себя',\n",
       " 'картина',\n",
       " 'ламборджини',\n",
       " 'дьябло',\n",
       " 'у',\n",
       " 'открывать',\n",
       " 'дверь',\n",
       " 'водитель',\n",
       " 'эффектный',\n",
       " 'девушка',\n",
       " '...',\n",
       " 'рядом',\n",
       " 'валяться',\n",
       " 'мужичок',\n",
       " 'весь',\n",
       " 'оборванный',\n",
       " 'и',\n",
       " 'с',\n",
       " 'фингал',\n",
       " '...',\n",
       " 'и',\n",
       " 'название',\n",
       " ' - \"',\n",
       " 'надавать',\n",
       " '....']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymystem3 import Mystem\n",
    "mystem = Mystem()\n",
    "\n",
    "def lemmatize2(text):\n",
    "    lemmas = mystem.lemmatize(text.lower())\n",
    "    lemmas = [lemma for lemma in lemmas if \n",
    "                 lemma != ' ' and lemma != '\\n' and \\\n",
    "                 lemma.strip() not in (string.punctuation + \"''``«»\")]\n",
    "    return lemmas\n",
    "\n",
    "\n",
    "lemmatize2(joke[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct and inverted index\n",
    "\n",
    "Direct index is list of dictionaries. Each dictionary element `d[word]` shows how many times word was used in corresponding joke.\n",
    "\n",
    "Inverted index maps words into list (t) of lists of good and bad joke indices. `t[0]` - list of bad jokes, `t[1]` - list of good jokes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateIndex(jokes_list):\n",
    "    direct = []\n",
    "    inverted = {}\n",
    "    for i in range(len(jokes_list)):\n",
    "        joke = jokes_list[i]\n",
    "        is_good = joke[-1]\n",
    "        tokens = tokenize(joke[0])\n",
    "        lemmas = [lemmatize(t) for t in tokens]\n",
    "        # direct index\n",
    "        lemmas_count = {}\n",
    "        for lemma in lemmas:\n",
    "            lemmas_count[lemma] = lemmas_count.get(lemma, 0) + 1\n",
    "        direct.append(lemmas_count)\n",
    "        # inverted index\n",
    "        lemmas_count = {}\n",
    "        for lemma in lemmas:\n",
    "            if lemma not in inverted:\n",
    "                inverted[lemma] = [[], []]\n",
    "                inverted[lemma][is_good] = [i]\n",
    "            elif lemma not in lemmas_count:  # If it is not repeated word in joke\n",
    "                inverted[lemma][is_good] += [i]\n",
    "            lemmas_count[lemma] = 1\n",
    "    return direct, inverted\n",
    "\n",
    "\n",
    "import winsound\n",
    "\n",
    "an_jokes, bash_jokes = LoadDatasets()\n",
    "\n",
    "an_jokes_direct, an_jokes_inverted = CreateIndex(an_jokes)\n",
    "bash_jokes_direct, bash_jokes_inverted = CreateIndex(bash_jokes)\n",
    "\n",
    "winsound.Beep(500, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('..\\\\data\\\\interim\\\\an_jokes_direct.json', 'w', encoding = 'utf-8')\n",
    "f.write(json.dumps(an_jokes_direct, ensure_ascii = False))\n",
    "f.close()\n",
    "f = open('..\\\\data\\\\interim\\\\an_jokes_inverted.json', 'w', encoding = 'utf-8')\n",
    "f.write(json.dumps(an_jokes_inverted, ensure_ascii = False))\n",
    "f.close()\n",
    "f = open('..\\\\data\\\\interim\\\\bash_jokes_direct.json', 'w', encoding = 'utf-8')\n",
    "f.write(json.dumps(bash_jokes_direct, ensure_ascii = False))\n",
    "f.close()\n",
    "f = open('..\\\\data\\\\interim\\\\bash_jokes_inverted.json', 'w', encoding = 'utf-8')\n",
    "f.write(json.dumps(bash_jokes_inverted, ensure_ascii = False))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
